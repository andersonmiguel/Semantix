#########################################################################                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   
Perguntas e Respostas do Questionário                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       
#########################################################################                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   
1. Qual o objetivo do comando cache em Spark?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
Seu objetivo principal é armazenar resultados de operações presentes no código em memória para reutilização durante o fluxo de processamento, evitando diversos acessos aos discos para uma mesma instrução. Muitas operações em um RDD são denominadas lazy, que significa abstrações para um conjunto de instruções a serem executadas. Se não utilizarmos o comando cache para essas instruções, teremos um aumento de instruções de I/O nos discos, deixando a performance do nosso código mais baixa, além de aumentar os custos de processamento e armazenamento de áreas temporárias.
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
2. O mesmo código implementado em Spark é normalmente mais rápido que a implementação equivalente em MapReduce. Por quê?                                                                                                                                                                                                                                                                                                                                                                                                                                                                    
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
Seguindo com o mesmo principio da questão anterior, o MapReduce utiliza o armazenamento em disco para gravar suas interações (ou resultados temporários), aumentando o consumo de I/O e processamento, deixando o processo mais lento.                                                                                                                                                                                                                                                                                                                                                      
Spark, por outro lado, permite que resultados intermediários sejam passados diretamente entre as operações a serem executadas através do caching desses dados em memória, ou até mesmo que diversas operações possam ser executadas sobre um mesmo conjunto de dados em cache, reduzindo a necessidade de operações de I/O em disco. Um Adicionando mais um cenário, para cada job MapReduce executado, uma nova instância da JVM é iniciada, enquanto Spark mantém a JVM em constantemente em execução em cada nó, precisando apenas iniciar uma nova thread, ação bem mais performática.  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
3. Qual é a função do SparkContext ?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
O SparkContext funciona como um client do ambiente de execução Spark. Podemos criar variaveis, executar jobs, passar configurações de alocação de recursos como memória e executores, criar RDDs etc.                                                                                                                                                                                                                                                                                                                                                                                       
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
4. Explique com suas palavras o que é Resilient Distributed Datasets (RDD)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
Coleção de objetos somente leitura, particionados em um conjunto de nodes do cluster, podendo somente  ser criado através de funções determinísticas (map, filter, join, groupBy) executadas em outros RDDs ou meios de armazenamentos estáveis como o Hadoop Filesystem. Basicamente, suportam dois tipos de operações: Transformations (map, filter, join, union, etc) e Actions (reduce, count, first, etc).                                                                                                                                                                             
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
5. GroupByKey é menos eficiente que reduceByKey em grandes dataset. Por quê?                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            
reduceByKey faz uma agregação prévia em cada node antes de fazer uma agregação final, diminuindo o tráfego de dados entre os nodes.        

6. Explique o que o código Scala abaixo faz.                                                                                                                                                                                                                                                                                                                                                                                                                                                 
                                            
Cria um arquivo mostrando as ocorrencias de cada palavra do arquivo lido.
